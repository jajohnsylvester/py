{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db0e960",
   "metadata": {},
   "source": [
    "source: https://openai.github.io/openai-agents-python/context/\n",
    "# Context management\n",
    "\n",
    "1. Context available locally to your code: this is data and dependencies you might need when tool functions run, during callbacks like on_handoff, in lifecycle hooks, etc.\n",
    "2. Context available to LLMs: this is data the LLM sees when generating a response.\n",
    "\n",
    "## Local context\n",
    "Represented via the RunContextWrapper class and the context property within it. The way this works is:\n",
    "\n",
    "- You create any Python object you want. A common pattern is to use a dataclass or a Pydantic object.\n",
    "- You pass that object to the various run methods (e.g. Runner.run(..., **context=whatever**)).\n",
    "- All your tool calls, lifecycle hooks etc will be passed a wrapper object, RunContextWrapper[T], where T = context object type accessible via wrapper.context.\n",
    "\n",
    "The most important thing to be aware of: every agent, tool function, lifecycle etc for a given agent run **must use the same type of context.**\n",
    "\n",
    "You can use it for:\n",
    "\n",
    "- Contextual data for your run (e.g. things like a username/uid or other information about the user)\n",
    "- Dependencies (e.g. logger objects, data fetchers, etc)\n",
    "- Helper functions\n",
    "\n",
    "Note\n",
    "The context object is not sent to the LLM. It is purely a local object that you can read from, write to and call methods on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f710361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price of the product, SuperWidget, is $99.99.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from agents import Agent, RunContextWrapper, Runner, function_tool\n",
    "\n",
    "@dataclass\n",
    "class ProductInfo:  \n",
    "    product_name: str\n",
    "    product_id: int\n",
    "\n",
    "@function_tool\n",
    "async def fetch_product_price(wrapper: RunContextWrapper[ProductInfo]) -> str:  \n",
    "    \"\"\"Fetch the price of the product. Call this function to get product's price information.\"\"\"\n",
    "    return f\"The product {wrapper.context.product_name} costs $99.99\"\n",
    "\n",
    "\n",
    "product_info = ProductInfo(product_name=\"SuperWidget\", product_id=456)\n",
    "\n",
    "agent = Agent[ProductInfo](  \n",
    "    name=\"ProductAssistant\",\n",
    "    tools=[fetch_product_price],\n",
    ")\n",
    "\n",
    "# In a Jupyter notebook, you need to use 'await' for async functions,\n",
    "# unless you use an event loop runner like asyncio.run (not recommended in notebooks),\n",
    "# or use IPython's built-in 'await' support (which is available in modern Jupyter).\n",
    "# So, you should keep 'await' here:\n",
    "result = await Runner.run(  \n",
    "    starting_agent=agent,\n",
    "    input=\"What is the price of the product?\",\n",
    "    context=product_info,\n",
    ")\n",
    "\n",
    "print(result.final_output)  \n",
    "# The product SuperWidget costs $99.99."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118cb0ef",
   "metadata": {},
   "source": [
    "# Agent/LLM context\n",
    "\n",
    "To make new data available to an LLM, you must add it to the conversation history. Main ways to do this:\n",
    "\n",
    "- Add it to Agent instructions (system prompt), either as a static string or a dynamic function.\n",
    "- Include it in the input when calling Runner.run.\n",
    "- Expose it via function tools for on-demand access.\n",
    "- Use retrieval or web search tools to fetch relevant data as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can experiment with different ways to expose data to the LLM here:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
